--tensor-model-parallel-size 1 \
                     --pipeline-model-parallel-size 1 \
                     --num-layers 10 \
                     --hidden-size 8192 \
                     --ffn-hidden-size 28672 \
                     --num-attention-heads 64 \
                     --micro-batch-size 1 \
                     --global-batch-size 1024 \
                     --seq-length 4096 \
                     --max-position-embeddings 4096 \
                     --min-lr 1.0e-4 \
                     --clip-grad 1.0 \
                     --swiglu
                     --use-distributed-optimizer \
                     --untie-embeddings-and-output-weights \
                     --fp16